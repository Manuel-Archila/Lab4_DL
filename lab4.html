<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lab4</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="lab4_files/libs/clipboard/clipboard.min.js"></script>
<script src="lab4_files/libs/quarto-html/quarto.js"></script>
<script src="lab4_files/libs/quarto-html/popper.min.js"></script>
<script src="lab4_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="lab4_files/libs/quarto-html/anchor.min.js"></script>
<link href="lab4_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="lab4_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="lab4_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="lab4_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="lab4_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="laboratorio-4" class="level1">
<h1>Laboratorio 4</h1>
<p>Sean bienvenidos de nuevo al laboratorio 4 de Deep Learning y Sistemas Inteligentes. Así como en los laboratorios pasados, espero que esta ejercitación les sirva para consolidar sus conocimientos en el tema de Encoder-Decoder y AutoEnconders.</p>
<p>Para este laboratorio estaremos usando una herramienta para Jupyter Notebooks que facilitará la calificación, no solo asegurándo que ustedes tengan una nota pronto sino también mostrandoles su nota final al terminar el laboratorio.</p>
<p>Espero que esta vez si se muestren los <em>marks</em>. De nuevo me discupo si algo no sale bien, seguiremos mejorando conforme vayamos iterando. Siempre pido su comprensión y colaboración si algo no funciona como debería.</p>
<p>Al igual que en el laboratorio pasado, estaremos usando la librería de Dr John Williamson et al de la University of Glasgow, además de ciertas piezas de código de Dr Bjorn Jensen de su curso de Introduction to Data Science and System de la University of Glasgow para la visualización de sus calificaciones.</p>
<p><strong>NOTA:</strong> Ahora tambien hay una tercera dependecia que se necesita instalar. Ver la celda de abajo por favor</p>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:54.419993Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:54.409473Z&quot;}" data-execution_count="118">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Una vez instalada la librería por favor, recuerden volverla a comentar.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install -U --force-reinstall --no-cache https://github.com/johnhw/jhwutils/zipball/master</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install scikit-image</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install -U --force-reinstall --no-cache https://github.com/AlbertS789/lautils/zipball/master</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T22:24:37.953793Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T22:24:34.644956Z&quot;}" data-execution_count="119">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">#from IPython import display</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">#from base64 import b64decode</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Other imports</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> unittest.mock <span class="im">import</span> patch</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> uuid <span class="im">import</span> getnode <span class="im">as</span> get_mac</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jhwutils.checkarr <span class="im">import</span> array_hash, check_hash, check_scalar, check_string, array_hash, _check_scalar</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jhwutils.image_audio <span class="im">as</span> ia</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jhwutils.tick <span class="im">as</span> tick</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lautils.gradeutils <span class="im">import</span> new_representation, hex_to_float, compare_numbers, compare_lists_by_percentage, calculate_coincidences_percentage</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">###</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>tick.reset_marks()</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:55.567829Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:55.560965Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;29e52b805cfebe42903d0379a3f485da&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-95b81aaa3e57306b&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="120">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Seeds</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>seed_ <span class="op">=</span> <span class="dv">2023</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:55.581630Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:55.567829Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;3aa8961ba46ffd91e0ae666686e967e7&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-b2ae10e4b3198bb2&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="121">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Celda escondida para utlidades necesarias, por favor NO edite esta celda</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="información-del-estudiante-en-dos-variables" class="level6">
<h6 class="anchored" data-anchor-id="información-del-estudiante-en-dos-variables">Información del estudiante en dos variables</h6>
<ul>
<li>carne_1 : un string con su carne (e.g.&nbsp;“12281”), debe ser de al menos 5 caracteres.</li>
<li>firma_mecanografiada_1: un string con su nombre (e.g.&nbsp;“Albero Suriano”) que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)</li>
<li>carne_2 : un string con su carne (e.g.&nbsp;“12281”), debe ser de al menos 5 caracteres.</li>
<li>firma_mecanografiada_2: un string con su nombre (e.g.&nbsp;“Albero Suriano”) que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:55.588643Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:55.581630Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;e7c7bd38d70a53f41a59434e097ebf75&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-887917342d3eaa54&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="122">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># carne_1 = </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># firma_mecanografiada_1 = </span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># carne_2 = </span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># firma_mecanografiada_2 = </span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>carne_1 <span class="op">=</span> <span class="st">"161250"</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>firma_mecanografiada_1 <span class="op">=</span> <span class="st">"Manuel Archila"</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>carne_2 <span class="op">=</span> <span class="st">"20090"</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>firma_mecanografiada_2 <span class="op">=</span> <span class="st">"Juan Diego Avila"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:55.602639Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:55.588643Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;6069d482a40ebc901473d44861baeb63&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-4aa33cdbf61b184d&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="123">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Deberia poder ver dos checkmarks verdes [0 marks], que indican que su información básica está OK </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">0</span>): </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(carne_1)<span class="op">&gt;=</span><span class="dv">5</span> <span class="kw">and</span> <span class="bu">len</span>(carne_2)<span class="op">&gt;=</span><span class="dv">5</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">0</span>):  </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(firma_mecanografiada_1)<span class="op">&gt;</span><span class="dv">0</span> <span class="kw">and</span> <span class="bu">len</span>(firma_mecanografiada_2)<span class="op">&gt;</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"0"}--> 
         ✓ [0 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"0"}--> 
         ✓ [0 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="parte-1---word2vec" class="level2">
<h2 class="anchored" data-anchor-id="parte-1---word2vec">Parte 1 - Word2Vec</h2>
<p><strong>Créditos:</strong> La primera parte de este laboratorio está tomado y basado en uno de los post de Musashi (Jacobs-) Harukawa</p>
<p>La eficacia de las técnicas de embedding está directamente relacionada con los desafíos iniciales que motivaron los enfoques de texto como datos. Al convertir el lenguaje natural en representaciones numéricas, los métodos de incrustación abren oportunidades para aplicar varias herramientas cuantitativas a fuentes de datos previamente sin explotar.</p>
<p>En términos generales, word embedding representa cada palabra en un conjunto dado de textos (corpus) como vectores en un espacio k-dimensional (donde k es elegido por el investigador; más detalles sobre esto más adelante). Estos vectores contienen información valiosa sobre las relaciones de las palabras y su contexto, sirviendo como herramientas esenciales para las tareas posteriores de modelado del lenguaje.</p>
<p>Entonces, es entendible que se pregunten</p>
<ul>
<li>¿Cómo funciona este proceso de incrustación?</li>
<li>¿Cuál es la razón subyacente de su éxito?</li>
<li>¿Cómo podemos determinar su eficacia?</li>
</ul>
<p>Para poder responder las primeras dos preguntas, vamos a implementar este modelo usando PyTorch. Noten que el state-of-the-art ya no solo se usa Word2Vec, como BERT (Bidirectional Encoder Representations from Transformers). Pero siempre es un buen ejercicio entender estos algoritmos.</p>
<section id="paso-1---dataloader" class="level3">
<h3 class="anchored" data-anchor-id="paso-1---dataloader">Paso 1 - DataLoader</h3>
<p>Como en laboratorios, lo primero que necesitamos es definir un DataLoader. Para esta primera parte estaremos usando el dataset llamado “tweets_hate_speech_detection” de HugginFace.</p>
<p>Para esto necesitamos una función que separe los textos en listas de tokens. El preprocesamiento para cuando se trabaja con textos debe ser un poco más exhaustivo de lo que haremos en este laboratorio, pero para fines del mismo solamente haremos:</p>
<p>1- Pasar a minusculas</p>
<p>2- Quitar todos los simbolos diferentes de a-z@#</p>
<p>3- Separar en espacios</p>
<p>4- Quitar “stopword” y tokens vacíos</p>
<p>5- Aplicar snowball stemmer al resto (snowball? sí, refieran a la nota de abajo para la explicación rápida)</p>
<p>Para esto nos apoyaremos en el paquete de natural language processing toolkit o nltk para los cuates. Entonces, recuerden instalarlo por favor “pip install nltk”</p>
<p><strong>Snowball Stemmer</strong> es un modulo en la librería NLTK que implementa la técnica de stemming. ¿Stemming? Stemming es una técnica utilizada para extraer la forma base de las palabras mediante la eliminación de los (pre-post)fijos de ellos. Imaginen que cortan la ramas de un árbol hasta los tallos. Por ejemplo, la raíz de las palabras comiendo, come, comido es comer. Refieran a este <a href="https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_stemming_lemmatization.htm">link</a> para más información</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:58.840015Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:55.602639Z&quot;}" data-execution_count="124">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> datasets.load_dataset(<span class="st">'tweets_hate_speech_detection'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:59.004447Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:58.840015Z&quot;}" data-execution_count="125">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Para simplicidad quitemos characteres pero mantegamos @ y #</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem.snowball <span class="im">import</span> SnowballStemmer</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>ss <span class="op">=</span> SnowballStemmer(<span class="st">'english'</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>sw <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">#def split_tokens(row):                             # PASO</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">#    row['all_tokens'] = [ss.stem(i) for i in       # 5</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">#                     re.split(r" +",               # 3</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">#                     re.sub(r"[^a-z@# ]", "",      # 2</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co">#                            row['tweet'].lower())) # 1</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co">#                     if (i not in sw) and len(i)]  # 4</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co">#    return row</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_tokens(row):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1- Pasar a minusculas</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    tweet_lower <span class="op">=</span> row[<span class="st">'tweet'</span>].lower()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2- Quitar todos los simbolos diferentes de a-z@#</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    tweet_cleaned <span class="op">=</span> re.sub(<span class="vs">r"[^a-z@# ]"</span>, <span class="st">""</span>, tweet_lower)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3- Separar en espacios</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    tweet_tokens <span class="op">=</span> re.split(<span class="vs">r" +"</span>, tweet_cleaned)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4- Quitar "stopword" y tokens vacíos</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5- Aplicar snowball stemmer al resto </span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    filtered_tokens <span class="op">=</span> [ss.stem(i) <span class="cf">for</span> i <span class="kw">in</span> tweet_tokens <span class="cf">if</span> (i <span class="kw">not</span> <span class="kw">in</span> sw) <span class="kw">and</span> <span class="bu">len</span>(i)]</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    row[<span class="st">'all_tokens'</span>] <span class="op">=</span> filtered_tokens</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> row</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\archi\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:59.025478Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:59.004447Z&quot;}" data-execution_count="126">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Determinamos el vocabulario</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.<span class="bu">map</span>(split_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ahora podemos crear algunas variables que nos serán útiles en futuros pasos. Además, debemos quitar los tokens que ocurren menos de 10 veces para reducir el tamaño del vocabulario</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:59.486408Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:59.025478Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;0d1e8d5c6985acc13aa5f87a951182e5&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-0305720eb97e48ae&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="127">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Total de palabras</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> Counter([i <span class="cf">for</span> s <span class="kw">in</span> dataset[<span class="st">'train'</span>][<span class="st">'all_tokens'</span>] <span class="cf">for</span> i <span class="kw">in</span> s])</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> {k:v <span class="cf">for</span> k, v <span class="kw">in</span> counts.items() <span class="cf">if</span> v<span class="op">&gt;</span><span class="dv">10</span>} <span class="co"># Filtering</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Aprox 1 linea para obtener los tokens unicos</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># vocab = </span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Hint: Use list de python</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Hint2: Use la variable counts</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> <span class="bu">list</span>(counts.keys())</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Aprox 1 linea para determinar el tamaño del vocabulario</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>n_v <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Aprox 2 lineas para definir </span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">#     los diccionarios para ir de un token a un id numérico y viceversa</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co"># id2tok = </span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># tok2id = </span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Hint: Puede que dict y enumerate le sirva para una definición</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>id2tok <span class="op">=</span> {i:tok <span class="cf">for</span> i, tok <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)}</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>tok2id <span class="op">=</span> {tok:i <span class="cf">for</span> i, tok <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)}</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Funcion para quitar tokens "raros"</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_rare_tokens(row):</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    row[<span class="st">'tokens'</span>] <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> row[<span class="st">'all_tokens'</span>] <span class="cf">if</span> t <span class="kw">in</span> vocab]</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> row</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.<span class="bu">map</span>(remove_rare_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:59.501418Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:59.487438Z&quot;}" data-execution_count="128">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="128">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['label', 'tweet', 'all_tokens', 'tokens'],
        num_rows: 31962
    })
    test: Dataset({
        features: ['label', 'tweet', 'all_tokens', 'tokens'],
        num_rows: 17197
    })
})</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:59.517184Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:59.502434Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;d4cf221bfa47a4f32352470e32c09b04&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-801ad99c67585892&quot;,&quot;locked&quot;:true,&quot;points&quot;:18,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="129">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(<span class="bu">len</span>(counts), <span class="st">'0xf4f4eb83'</span>))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(<span class="bu">len</span>(id2tok), <span class="st">'0xf4f4eb83'</span>))</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(<span class="bu">len</span>(vocab), <span class="st">'0xf4f4eb83'</span>))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(n_v, <span class="st">'0xf4f4eb83'</span>))</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(tok2id[<span class="st">'father'</span>], <span class="st">'0xb44c37ea'</span>))</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_string(id2tok[<span class="dv">1</span>], <span class="st">'0xcf2531b8'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
</div>
<p>Ahora, recordemos que Word2Vec ayuda a representar una palabra por su contexto, para ello necesitamos definir una ventana movil (sliding window) que se usa dentro del algoritmo. Esta consiste en tomar cada palabra de una frase, y luego se parea con las N palabras más cercanas (hacia la derecha e izquierda). Por ejemplo, consideremos una frase como “every good dog does fine”, con una ventana de 2. El resultado sería algo como:</p>
<p><code>(every, good)</code> <code>(every, dog)</code> <code>(good, every)</code> <code>(good, dog)</code> <code>(good, does)</code> <code>(dog, every)</code> <code>(dog, good)</code> <code>...</code></p>
<p>Y así consecutivamente. La frase u oración, es convertida en un par <code>target, context</code> donde el contex es una lista de tokens dentro de la ventana.</p>
<p>Luego, definiremos el DataSet usando las clases correspondiente como lo hemos hecho antes.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:29:59.580001Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:29:59.520187Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;f3be587ebad6002016b9a1210d7dfb2d&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-7d04d85a04e5fc7a&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="130">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#def windowizer(row, wsize=3):</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co">#    """</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">#    Windowizer function for Word2Vec. Converts sentence to sliding-window</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co">#    pairs.</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">#    """</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">#    doc = row['tokens']</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">#    #wsize = 3</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">#    out = []</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">#    for i, word in enumerate(doc):</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">#        target = tok2id[word]</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">#        window = [i+j for j in</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">#                  range(-wsize, wsize+1, 1)</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">#                  if (i+j&gt;=0) &amp;</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">#                     (i+j&lt;len(doc)) &amp;</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co">#                     (j!=0)]</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co">#        out += [(target, tok2id[doc[w]]) for w in window]</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co">#    row['moving_window'] = out</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co">#    return row</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> windowizer(row, wsize<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Windowizer function for Word2Vec. Converts sentence to sliding-window</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="co">    pairs.</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    doc <span class="op">=</span> row[<span class="st">'tokens'</span>]</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(doc):</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> tok2id[word]</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1 - Definimos el rango de la ventana movil</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        window <span class="op">=</span> [i <span class="op">+</span> j <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="op">-</span>wsize, wsize <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>) <span class="cf">if</span> (i <span class="op">+</span> j <span class="op">&gt;=</span> <span class="dv">0</span>) <span class="op">&amp;</span> (i <span class="op">+</span> j <span class="op">&lt;</span> <span class="bu">len</span>(doc)) <span class="op">&amp;</span> (j <span class="op">!=</span> <span class="dv">0</span>)]</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2 - Creamos pares de la ventana movil</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># window_pairs = </span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>        window_pairs <span class="op">=</span> [(target, tok2id[doc[w]]) <span class="cf">for</span> w <span class="kw">in</span> window]</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3 - Agregamos los pares a la lista de salida</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea </span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out +=</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>        out <span class="op">+=</span> window_pairs</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4 - Asingamos el "movin_window" a la fila</span></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>    row[<span class="st">'moving_window'</span>] <span class="op">=</span> out</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> row</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.<span class="bu">map</span>(windowizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Map: 100%|██████████| 31962/31962 [00:07&lt;00:00, 4419.77 examples/s]
Map: 100%|██████████| 17197/17197 [00:03&lt;00:00, 4423.16 examples/s]</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:30:02.539396Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:30:02.518455Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;726e16c824a2e5b29968e3f1bbab59bf&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-66be60a252d35f9d&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="131">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(dataset[<span class="st">"train"</span>].num_rows, <span class="st">'0xcd61d16b'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:30:02.555506Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:30:02.539396Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;8c728e41718e96a23aa6a0f9ce216263&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1d4097cccc4ceee4&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="132">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">#from torch.utils.data import Dataset, DataLoader</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, Dataset, DataLoader</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Word2VecDataset(Dataset):</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dataset, vocab_size, wsize<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dataset <span class="op">=</span> dataset</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> [i <span class="cf">for</span> s <span class="kw">in</span> dataset[<span class="st">'moving_window'</span>] <span class="cf">for</span> i <span class="kw">in</span> s]</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.data[idx][<span class="dv">0</span>], <span class="va">self</span>.data[idx][<span class="dv">1</span>]</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ahora definiremos dos variables globales, el <code>BATCH_SIZE</code> y <code>N_LOADER_PROCS</code>.</p>
<p><code>BATCH_SIZE</code> es el número de observaciones devueltas con cada llamada. Gran parte de las aceleraciones del procesamiento de GPU provienen de cálculos de matriz por batches masivos. Al elegir el tamaño del batch, recuerden que generalmente se trata de un trade-off entre el uso de VRAM y la velocidad, excepto cuando el Data Loader en sí es el cuello de botella. Para acelerar el DataLoader, podemos pasar un argumento a num_workers para habilitar la paralelización en la preparación y carga de datos.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:30:05.406175Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:30:02.556990Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;de6b1867f45bd4673c2fe7c464049ae0&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-12cd6ba3b1e9f944&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="133">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of the Word2VecDataset</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>word2vec_dataset_ <span class="op">=</span> Word2VecDataset(dataset[<span class="st">'train'</span>], vocab_size<span class="op">=</span>n_v)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the Word2VecDataset into a TensorDataset</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>word2vec_dataset <span class="op">=</span> TensorDataset(torch.tensor(word2vec_dataset_.data, dtype<span class="op">=</span>torch.<span class="bu">long</span>))</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">16</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>N_LOADER_PROCS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>dataloader_train <span class="op">=</span> DataLoader(word2vec_dataset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span>N_LOADER_PROCS)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="paso-2---construyendo-la-red" class="level3">
<h3 class="anchored" data-anchor-id="paso-2---construyendo-la-red">Paso 2 - Construyendo la Red</h3>
<p>La arquitectura que usaremos para esta ocasión será la dada por una versión de Word2Vec, esta consiste en: * Tres capas: Input, hidden y output * Tanto el tamaño de la input como la output son del tamaño del vocabulario. Pero la hidden es un poco más pequeña * Todas son Fully Connected con Funciones de Activación Lineales</p>
<p>Como mencionamos en clase hay dos variantes * CBOW (Continuous Bag of Words): El enfoque está dado en las palabras de contexto para dar énfasis a la palabra central. O en otras palabras, las palabras de contexto son el input y la palabra central son el output (Espero que esto haga más sentido de la explicación en clase) * Skip-gram: La palabra central es el input, y las de contexto son la salida.</p>
<p>Definamos CBOW para este laboratorio…</p>
<p>Pero antes, debemos encodear nuestras palabras (otra vez como lo hicimos en el laboratorio pasado), esta implementación es similar a la que hicimos anteriormente, pero observen el uso de tensores.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:30:05.419716Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:30:05.412780Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;79bcbc0e794ad778277522e74da6198f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-719345b22d8a6412&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="134">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>input_ <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode(input_, size):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    vec <span class="op">=</span> torch.zeros(size).<span class="bu">float</span>()</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># vec[input_] =</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    vec[input_] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vec</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>ohe <span class="op">=</span> one_hot_encode(input_, size)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>linear_layer <span class="op">=</span> nn.Linear(size, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:30:05.426787Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:30:05.419716Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;b9076aa87b82f56230dfbf2cec1760ad&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-87b60412b0ba69d1&quot;,&quot;locked&quot;:true,&quot;points&quot;:3,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="135">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">int</span>(ohe[<span class="dv">7</span>])  <span class="op">==</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
</div>
<p>Ahora, sobreescribamos el comportamiento natural de la inicializacion de pesos, para que estos en lugar de iniciar aleatoriamente, sean valores de 0 - size. Esto lo hacemos dentro <code>torch.no_grad()</code> para quitar el tracking de la gradiente (recuerden que cuando usamos los tensores de PyTorch la gradiente se le hace tracking, es decir que se almacenan para hacer la diferenciar la pérdida con respecto de cada parametro en el modelo. Debido a que en esta ocasion lo estamos seteando manualmente no queremos que se almacene y sea considerado en futuras backpropagations.</p>
<p>Observen como al pasar el vector encodeado a la capa nos devuelve efectivamente el número que corresponde en <code>linear_layer(ohe)</code></p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:30:05.440557Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:30:05.426787Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7d26f4b5566a553c62c142900b507a20&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-4c85a0ffbffa8e55&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="136">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    linear_layer.weight <span class="op">=</span> nn.Parameter(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        torch.arange(size, dtype<span class="op">=</span>torch.<span class="bu">float</span>).reshape(linear_layer.weight.shape))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(linear_layer.weight)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(linear_layer(ohe))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,
         14., 15., 16., 17., 18., 19.]], requires_grad=True)
tensor([7.], grad_fn=&lt;SqueezeBackward4&gt;)</code></pre>
</div>
</div>
<p>Ya que tenemos un mejor entendimiento de este tipo de layers en Word2Vec, debemos saber que PyTorch tiene una implementación más eficiente usando <code>nn.Embedding</code>, el cual toma los índices de input y regresa el peso del borde correspondiente a ese índice.</p>
<p>Un equivalente a lo que hemos hecho anteriormente sería lo que se presenta en la siguiente celda.</p>
<p>Noten como volvemos a obtener un tensor similar al que obtuvimos antes.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:30:05.463215Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:30:05.440557Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;bc320f7bb25be19e4e0033321279d77c&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-bf93477666a5691e&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="137">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> nn.Embedding(size, <span class="dv">1</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    embedding_layer.weight <span class="op">=</span> nn.Parameter(</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        torch.arange(size, dtype<span class="op">=</span>torch.<span class="bu">float</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        ).reshape(embedding_layer.weight.shape))</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedding_layer.weight)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedding_layer(torch.tensor(input_)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor([[ 0.],
        [ 1.],
        [ 2.],
        [ 3.],
        [ 4.],
        [ 5.],
        [ 6.],
        [ 7.],
        [ 8.],
        [ 9.],
        [10.],
        [11.],
        [12.],
        [13.],
        [14.],
        [15.],
        [16.],
        [17.],
        [18.],
        [19.]], requires_grad=True)
tensor([7.], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
<p>Con esto en consideración, es momento de implementar nuestro modelo Word2Vec.</p>
<p>Noten el embedding_size, este corresponde a la cantidad de representaciones de cada palabra, como dijimos en clase, esto sería la cantidad de funciones de activaciones con las que trabajaremos.</p>
<p>Además, consideren las siguientes explicaciones</p>
<p><code>self.embed</code>: Es una capa de embedding para convertir la entrada (el índice del token de centro/contexto) en la codificación one-hot, y luego recuperar los pesos correspondientes a estos índices en la capa hidden de menor dimensión.</p>
<p><code>self.expand</code>: Es una capa lineal para predecir la probabilidad de una palabra de centro/contexto dada la hidden layer. Deshabilitamos el bias (la intercepción) porque cambiamos la escala de nuestras predicciones de todos modos.</p>
<p><code>logits</code>: Este vuelve a expandir la capa hidden para hacer predicciones. Estas predicciones sin procesar deben volver a escalarse con softmax, pero omitimos este paso aquí, ya que PyTorch implementa los pasos relevantes en la Cross Entropy loss.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T06:30:05.479149Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T06:30:05.465210Z&quot;}" data-execution_count="138">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Word2Vec(nn.Module):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_size):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed <span class="op">=</span> nn.Embedding(vocab_size, embedding_size)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.expand <span class="op">=</span> nn.Linear(embedding_size, vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pasamos el input a una representación más pequeña</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> <span class="va">self</span>.embed(input_)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expandemos hacia las predicciones</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.expand(hidden)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="paso-3---entrenamiento-training" class="level3">
<h3 class="anchored" data-anchor-id="paso-3---entrenamiento-training">Paso 3 - Entrenamiento (Training)</h3>
<p>El entrenamiento en el contexto de las redes neuronales significa hacer predicciones repetidamente utilizando las observaciones en el conjunto de datos y luego ajustar los parámetros para corregir el error en las predicciones.</p>
<p>Debido a que no queremos que la red aprenda perfectamente la predicción más reciente mientras olvida todas las demás predicciones, generalmente le damos un “learning rate”, que es una penalización en el ajuste de pérdida para evitar que se ajuste solo a la observación más reciente. (Recuerden como funciona backpropgation)</p>
<p>Cuanto más tiempo entrenemos la red, con mayor perfección aprenderá los datos de entrenamiento, pero a menudo esto conlleva el riesgo de overfitting y no poder generalizar a datos no vistos. Sin embargo, dado que con Word2Vec nuestro objetivo no es inferir datos no vistos, sino describir datos “vistos”, ¿cuál creen que es la implicación del overfitting en este tipo de modelos? (Más adelante se deja nuevamente la pregunta para que sea respondida)</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T22:14:29.330767Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T22:14:29.296636Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;1a82d8c2a18f8548fa5a4b5765fb2fc1&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-588eec0490d68d93&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="139">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Algunos hyper parametros</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Demasiado pequeño pero es solo para fines de aprendizaje</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>EMBED_SIZE <span class="op">=</span> <span class="dv">50</span> </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(n_v, EMBED_SIZE)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Traten de usar ya el CUDA si pueden por favor</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span>) <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.device(<span class="st">'cpu'</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Using:"</span>,device)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Otros parametros para el training</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> <span class="fl">3e-4</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Noten el tipo de optimizador que estamos usando :)</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>LR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using: cpu</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T22:56:25.851356Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T22:14:39.282185Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;469a6891ae3c2d3ef10bfe706471264f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-8a89fd3a288c223a&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="140">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>running_loss <span class="op">=</span> []</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ix, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader_train):</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Working with batch </span><span class="sc">{</span>ix<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(batch[<span class="dv">0</span>])):</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>            center <span class="op">=</span> batch[<span class="dv">0</span>][i][<span class="dv">0</span>]</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>            context <span class="op">=</span> batch[<span class="dv">0</span>][i][<span class="dv">1</span>]</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>            center, context <span class="op">=</span> center.to(device), context.to(device)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Aprox 1 linea para </span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># optimizer.zero...</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># YOUR CODE HERE</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(input_<span class="op">=</span>context)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Aprox 1 linea para</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># loss = loss_....</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># YOUR CODE HERE</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(logits, center)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>            losses.append(loss.item())</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i<span class="op">%</span> <span class="dv">6500</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Done working with element </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> np.mean(losses)</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    running_loss.append(epoch_loss)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mostrar la perdida cada N epocas</span></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoca </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>epoch_loss<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Working with batch 0
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 1
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 2
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 3
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 4
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 5
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 6
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 7
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 8
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 9
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 10
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 11
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 12
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 13
Done working with element 0
Epoca 0, loss: 6.872251292369913
Working with batch 0
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 1
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 2
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 3
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 4
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 5
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 6
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 7
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 8
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 9
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 10
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 11
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 12
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 13
Done working with element 0
Epoca 1, loss: 6.771715011854427
Working with batch 0
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 1
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 2
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 3
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 4
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 5
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 6
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 7
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 8
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 9
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 10
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 11
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 12
Done working with element 0
Done working with element 6500
Done working with element 13000
Done working with element 19500
Done working with element 26000
Done working with element 32500
Done working with element 39000
Done working with element 45500
Done working with element 52000
Done working with element 58500
Done working with element 65000
Working with batch 13
Done working with element 0
Epoca 2, loss: 6.74554601581208</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T00:08:24.732102Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T00:08:24.725620Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;32ff5c07dd800c1cc3e4b2446f56cded&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-5fe835d2e4773764&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="141">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(running_loss[<span class="bu">len</span>(running_loss)<span class="op">-</span><span class="dv">1</span>]), <span class="st">"3c3d"</span>, <span class="st">'0x1.b000000000000p+2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-06T23:47:02.588400Z&quot;,&quot;start_time&quot;:&quot;2023-08-06T23:47:02.385345Z&quot;}" data-execution_count="142">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficamos la perdida</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>epoch_ <span class="op">=</span> np.arange(<span class="bu">len</span>(running_loss))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_, running_loss, <span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Loss'</span>,)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>), plt.ylabel(<span class="st">'NLL'</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lab4_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Bueno, hemos visto la cantidad de tiemp que hay que invertirle para entrenar una red tan sencilla como la que se usa en Wor2Vec. En mi caso, usando CUDA le tomó alrededor de <strong>42 minutos</strong>. Ahora consideren aquel modelo donde no solo se sacan 50 representaciones de cada palabra sino miles, además que se entrenan por más epocas, no solo 3.</p>
<p>Ahora veamos que tipo de palabras son las más cercanas a una pequeña muestra de 4 palabras. Para esto primero necesitamos sacar los pesos del modelo y pasarlos al cpu para trabajarlos como NumPy Arrays. Luego aplicaremos una función para encontrar la distancia dada una métrica (en este caso la distancia del coseno).</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T00:38:03.418312Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T00:38:03.384629Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;adf71f64fc49e300a12791c23246a50b&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-79c39d4b13d3ed81&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="143">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>wordvecs <span class="op">=</span> model.expand.weight.cpu().detach().numpy()</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [<span class="st">'good'</span>, <span class="st">'bad'</span>, <span class="st">'school'</span>, <span class="st">'day'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T00:38:23.499862Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T00:38:23.360937Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;6e097e2559d99154cefeabf845d4aa0d&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1d82d3393549def7&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="144">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_distance_matrix(wordvecs, metric):</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    dist_matrix <span class="op">=</span> distance.squareform(distance.pdist(wordvecs, metric))</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dist_matrix</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_k_similar_words(word, dist_matrix, k<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 2 lineas para</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># idx = ...</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># dists = ... # Use la funcion dada arriba</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hint: tok2id</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> tok2id[word]</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    dists <span class="op">=</span> dist_matrix[idx]</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.argpartition(dists, k)[:k<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> ind[np.argsort(dists[ind])][<span class="dv">1</span>:]</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> [(i, id2tok[i], dists[i]) <span class="cf">for</span> i <span class="kw">in</span> ind]</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>dmat <span class="op">=</span> get_distance_matrix(wordvecs, <span class="st">'cosine'</span>)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> tokens:</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(word, [t[<span class="dv">1</span>] <span class="cf">for</span> t <span class="kw">in</span> get_k_similar_words(word, dmat)], <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>good ['got', 'great', 'us', 'night', 'come', 'week', 'year', 'first', 'know', 'feel'] 

bad ['boy', 'talk', 'hear', 'doesnt', 'news', 'fan', 'person', 'stori', 'wont', 'everyth'] 

school ['soon', 'job', 'left', 'two', 'sinc', 'open', 'team', 'long', 'tell', 'ill'] 

day ['amp', '@user', 'happi', 'go', 'get', 'im', 'love', 'today', 'like', 'make'] 
</code></pre>
</div>
</div>
<p><strong>PREGUNTAS:</strong> * ¿Cuál es la implicación del overfitting en modelos como Word2Vec? * Ocurre cuando el modelo se ajusta demasiado a los datos de entrenamiento, por lo que no es capaz de generalizar bien a nuevos datos. En el caso de Word2Vec, se puede dar cuando da mucho peso a ocurrencias raras o específicas que no son relevantes para el contexto general de las palabras. * ¿Qué tan bien encontró palabras cercanas su modelo Word2Vec? ¿Podría mejorar? ¿Cómo podría mejorar? * El modelo encontró palabras cercanas, pero no todas las que se esperaban. Se podría mejorar aumentando el tamaño de entrenamiento, o aumentando el tamaño del vector de palabras. Aunque esto ayudaria a encontrar palabras más cercanas, también aumentaría el tiempo de entrenamiento. * A grandes rasgos, ¿cuál es la diferencia entre Word2Vec y BERT? * Word2Vec: aprende representaciones vectoriales para palabras segun su contexto. * BERT: Aprende considerando el contexto dentro de la oracion y se enfoca en la comprensión y generación de lenguaje según el contexto en el que se encuentran las palabras, esto incluye el contexto anterior y el contexto posterior.</p>
</section>
</section>
<section id="parte-2---encoder---decoder" class="level2">
<h2 class="anchored" data-anchor-id="parte-2---encoder---decoder">Parte 2 - Encoder - Decoder</h2>
<p><strong>Créditos:</strong> La segunda parte de este laboratorio está tomado y basado en uno de los repositorios de Ben Trevett</p>
<p>En esta ocasión vamos a centrarnos en una arquitectura Sequence to Sequence (Seq2Seq), entonces estaremos desarrollando un modelo que nos ayude a traducir de alemán a inglés. Tomaremos como base el paper <a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>. Recuerden que a pesar que esto es para frases/oraciones, los conceptos pueden ser aplicados para otras arquitecturas similares.</p>
<p><strong>IMPORTANTE:</strong> Recuerden usar virtual enviroments debido a que estaremos usando versiones viejas de la librerías. ¿Por qué? Las librerías eran un poco más explícitas que sus versiones más recientes. A continuación se dejan los comandos para la instalación de las más importantes</p>
<pre><code>pip install -U torch==1.9.0+cu111 -f  https://download.pytorch.org/whl/cu111/torch_stable.html
pip install -U torchtext==0.10.0</code></pre>
<p>El primer comando instalará la librería de PyTorch con CUDA 11.1 El segundo, instala TorchText en una versión donde la formulación del vocabulario para training, test y validation era más claro (esta es la principal por la que estamos usando esta versiones).</p>
<section id="introducción" class="level3">
<h3 class="anchored" data-anchor-id="introducción">Introducción</h3>
<p>Los modelos más comunes seq2seq son los modelos <em>encoder-decoder</em>, los cuales usan una RNN para encodear el input y llevarlo a un solo vector. En este laboratorio nos estaremos refiriendo a dicho vector como <em>vector contexto</em>. Pensemos sobre el vector contexto como un ser abstracto que representa una frase completa. Este vector es luego decodeado por una segunda RNN, que aprende a generar la frase target (output) deseada al generar palabra por palabra.</p>
<p>Consideren la siguiente ilustración para representar el proceso que estaremos realizando</p>
<p><img src="https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/49df8404d938a6edbf729876405558cc2c2b3013/assets/seq2seq1.png" alt="Seq2Seq"></p>
<p><em>Crédito de imagen al autor, imagen tomada de “Sequence to Sequence Learning with Neural Networks” de Ben Trevett</em></p>
<p>Noten como la frase de input “guten morgen”, se pasa a través de una capa de embedding (cuadros amarillos) y luego entra en los encodeadores (cuadros verdes). En esta ocasión agregamos un token de “start of sequence” (<code>&lt;sos&gt;</code>) al inicio de la frase, además de un token de “end of sequence” (<code>&lt;eos&gt;</code>) al final de la oración. Vean como en cada paso, la entrada del encoder RNN es tanto la representación embedding <span class="math inline">\(e\)</span> de la palabra actual <span class="math inline">\(e(x_t)\)</span>, así como el estado oculto del paso anterior <span class="math inline">\(h_{t-1}\)</span>, y el encoder genera un nuevo hidden state <span class="math inline">\(h_t\)</span>. Entonces, podemos pensar en el hidden state como una representación vectorial de la oración hasta ese momento. La RNN se puede representar como una función de tanto <span class="math inline">\(e(x_t)\)</span> y <span class="math inline">\(h_{t-1}\)</span></p>
<p><span class="math display">\[h_t = \text{EncoderRNN}(e(x_t), h_{t-1})\]</span></p>
<p>Por favor noten que estamos usando el termino RNN de forma general en este contexto, puede ser cualquier arquitectura como LSTM o GRU.</p>
<p>Entonces estaremos trabajando con una secuencia como <span class="math inline">\(X = \{x_1, x_2, ..., x_T\}\)</span>, donde <span class="math inline">\(x_1 = &lt;sos&gt;\)</span>, <span class="math inline">\(x_2 = guten\)</span>, y así consecutivamente. El hidden state inicial <span class="math inline">\(h_0\)</span> es usualmente iniciado con ceros o con algún parametro pre-aprendido.</p>
<p>Una vez la palabra final <span class="math inline">\(X_T\)</span> ha pasado en la RNN a través de la embedding layer, usamos el hidden state final <span class="math inline">\(h_T\)</span> como vector de contexto. Es decir, <span class="math inline">\(h_T = z\)</span>. El cual será la representación vectorial de toda la oración.</p>
<p>Ahora que tenemos nuestro vector de contexto <span class="math inline">\(z\)</span>, podemos empezar a decodear para obtener la oración target, “good morning”. De nuevo, agregamos los tokens de inicio y fin de la secuencia de nuestra oración target. En cada paso, el input al decoder RNN (cuadros azules de la imagen) es la versión embedding <span class="math inline">\(d\)</span> de la palabra actual <span class="math inline">\(d(y_t)\)</span> así como también el hidden state del paso previo <span class="math inline">\(s_{t-1}\)</span>m donde el hidden state del decoder incial <span class="math inline">\(s_0\)</span> es el vector de contexto <span class="math inline">\(s_0 = z = h_T\)</span>, es decir, el hidden state decoder es el último hidden state encoder. Por ende, simlar al encoder, podemos representarlo como:</p>
<p><span class="math display">\[s_t = \text{DecoderRNN}(d(y_t), s_{t-1})\]</span></p>
<p>A pesar que el input embeeding layer <span class="math inline">\(e\)</span> y el target embedding layer <span class="math inline">\(d\)</span> están representados como cuadros amarillos en la imagen, como dijimos en clase, estas son dos embedding layers diferentes con sus propios parametros.</p>
<p>En el decoder, necestamos ir del hidden state a la palabra actual, por ello en cada paso usamos <span class="math inline">\(s_t\)</span> para predecir (a traves de pasarlo en una layer lineal, mostrada como cuadros morados) lo que se cree que es la siguiente palabra en la secuencia <span class="math inline">\(\hat{y}_t\)</span></p>
<p><span class="math display">\[\hat{y}_t = f(s_t)\]</span></p>
<p>Las palabras en el decoder son siempre generadas una después de la otra, con una por paso. Siempre usamos <code>&lt;sos&gt;</code> para el primer input del decodr <span class="math inline">\(y_1\)</span> y algumas veces usamos la palabra predicha por nuestro decoder, <span class="math inline">\(\hat{y}_{t-1}\)</span>. Que, como mencionamos en clase, se le llama <em>teacher forcing</em>.</p>
<p>Cuando estamos entrenando o probando nuestro modelo, siempre sabemso cuantas palabras hay en nuestra secuencia target, entonces nos detenemos de generar palabras una vez alcanzamos esa cantidad. Durante las fases de inferencia (uso del modelo en la “vida real”) seguimos generando palabras hasta que el modelo genere un token <code>&lt;eos&gt;</code> o después de una cierta cantidad de palabras dada. (Esto tambien lo mencionamoos en clase, es solo para refrescar los conceptos)</p>
<p>Una vez tengamos nuestra secuencia target predicha <span class="math inline">\(\hat{Y} = \{ \hat{y}_1, \hat{y}_2, ..., \hat{y}_T \}\)</span>, la comparamos contra nuestra secuencia target real. <span class="math inline">\(Y = \{ y_1, y_2, ..., y_T \}\)</span>, para calcular la perdida. Usamos esta pérdida para actualizar los parámetros del modelo, como bien hemos hecho en otras ocasiones.</p>
</section>
<section id="preparación-de-data" class="level3">
<h3 class="anchored" data-anchor-id="preparación-de-data">Preparación de Data</h3>
<p>Es momento de ponernos a manos a la obra. Estaremos programando nuestro modelo usando PyTorch y usando torchtext para ayudarnos a hacer todo el pre-procesamiento necesario. Ahora usaremos spaCy para ayudarnos en la tokenización de los datos</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:04.622634Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:09:55.185815Z&quot;}" data-execution_count="145">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.datasets <span class="im">import</span> Multi30k</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>train_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz"</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>val_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz"</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>test_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz"</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the URLs in the Multi30k module</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>Multi30k.urls <span class="op">=</span> (train_url, val_url, test_url)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data <span class="im">import</span> Field, BucketIterator</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Colocamos las semillas para tener resultados consistentes.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:12.966554Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:12.934552Z&quot;}" data-execution_count="146">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">1234</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>random.seed(SEED)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(SEED)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(SEED)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(SEED)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ahora, necesitamos crear un tokenizador. Un tokenizador es una función que ayudará a convertir un string de alguna frase u oración en una lista de tokens individuales. Consideren que en una frase como “good morning!” se tienen tres tokens, siendo cada uno “good”, “morning” y “!”, noten que a pesar que el signo de admiracion no se considera una palabra, sí se considera como un token.</p>
<p>Para la creación de nuestro tokenizador nos apoyaremos en spaCy, en este caso necesitamos los paquetes de aleman e inglés (se nombran abajo).</p>
<p>Para instalar spaCy necesitarán ejecutar en la cmd</p>
<pre><code>pip install spacy
python -m spacy download en_core_web_sm
python -m spacy download de_core_news_sm</code></pre>
<p><strong>IMPORTANTE:</strong> Recuerden usar virtual environments de Python, debido a que este laboratorio usa algunas librerías deprecadas, que como se explicó previamente, se hizo de este modo para ser más explícito el aprendizaje.</p>
<p>Regresando al tema del tokenizer, primero cargaremos las dos versiones para los diferentes idiomas con los que estamos trabajando.</p>
<p>Despues, crearemos unas funciones de tokenización. Estas pueden ser pasadas a TorchText y tomarán una oración y regresara la oración como una lista de tokens.</p>
<p>Cabe la pena mencionar que en el paper que estamos tomando de base, ellos encontrarón util el revertir el orden del input dado que se cree que introducía varias dependencias a corto plazo en los datos que facilitan mucho el problema de optimización.</p>
<p>Más adelante, usaremos <code>Field</code> (que actualmente está deprecado :( ) para manejar como la data debería ser procesada. Después, seteamos el parametro <code>tokenize</code> como función para cada caso. El aleman será el <code>SRC</code> y el inglés será el <code>TRG</code>. Además también se agrega el token para inicio y fin de la secuencia, además que convertirá todo en lowercase.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:15.907822Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:14.202661Z&quot;}" data-execution_count="147">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>spacy_de <span class="op">=</span> spacy.load(<span class="st">'de_core_news_sm'</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>spacy_en <span class="op">=</span> spacy.load(<span class="st">'en_core_web_sm'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:16.426717Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:16.410680Z&quot;}" data-execution_count="148">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_de(text):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Tokenizes German text from a string into a list of strings (tokens) and reverses it</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [tok.text <span class="cf">for</span> tok <span class="kw">in</span> spacy_de.tokenizer(text)][::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_en(text):</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Tokenizes English text from a string into a list of strings (tokens)</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [tok.text <span class="cf">for</span> tok <span class="kw">in</span> spacy_en.tokenizer(text)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:16.775400Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:16.767392Z&quot;}" data-execution_count="149">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>SRC <span class="op">=</span> Field(tokenize <span class="op">=</span> tokenize_de, </span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>            init_token <span class="op">=</span> <span class="st">'&lt;sos&gt;'</span>, </span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>            eos_token <span class="op">=</span> <span class="st">'&lt;eos&gt;'</span>, </span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>            lower <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>TRG <span class="op">=</span> Field(tokenize <span class="op">=</span> tokenize_en, </span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>            init_token <span class="op">=</span> <span class="st">'&lt;sos&gt;'</span>, </span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>            eos_token <span class="op">=</span> <span class="st">'&lt;eos&gt;'</span>, </span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>            lower <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ahora, debemos descargar el dataset. Para este caso estaremos usando el dataset llamado Multi30k. Este tiene aproximadamente 30K frases en inglés, aleman y francés, cada uno tiene alrededor de 12 palabras por frase.</p>
<p>Además noten que <code>exts</code> especifica cual lenguage se debe usar como source y target, y <code>fields</code> da cuales campos usar para el source y target.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:21.558532Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:17.137695Z&quot;}" data-execution_count="150">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>train_data, valid_data, test_data <span class="op">=</span> Multi30k.splits(exts <span class="op">=</span> (<span class="st">'.de'</span>, <span class="st">'.en'</span>), </span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>                                                    fields <span class="op">=</span> (SRC, TRG),</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>                                                    root<span class="op">=</span><span class="st">'./data'</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:21.990868Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:21.982854Z&quot;}" data-execution_count="151">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Numero de observaciones de training: </span><span class="sc">{</span><span class="bu">len</span>(train_data.examples)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Numero de observaciones en validation: </span><span class="sc">{</span><span class="bu">len</span>(valid_data.examples)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Numero de observaciones en test: </span><span class="sc">{</span><span class="bu">len</span>(test_data.examples)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Numero de observaciones de training: 29000
Numero de observaciones en validation: 1014
Numero de observaciones en test: 1000</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:23.001453Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:22.753770Z&quot;}" data-execution_count="152">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>SRC.build_vocab(train_data, min_freq <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>TRG.build_vocab(train_data, min_freq <span class="op">=</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:22.369708Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:22.353942Z&quot;}" data-execution_count="153">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">vars</span>(train_data.examples[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}</code></pre>
</div>
</div>
<p>Observen como el punto está al comienzo de la oración en alemán (src), por lo que parece que la oración se invirtió correctamente.</p>
<p>Ahora, construiremos el vocabulario para los idiomas de source y de target. El vocabulario se utiliza para asociar cada token único con un índice (un número entero). Los vocabularios de los idiomas de origen y de destino son distintos.</p>
<p>Usando el argumento <code>min_freq</code>, solo permitimos que aparezcan en nuestro vocabulario tokens que aparecen al menos 2 veces. Los tokens que aparecen solo una vez se convierten en un token desconocido <code>&lt;unk&gt;</code>.</p>
<p>Es importante tener en cuenta que nuestro vocabulario solo debe construirse a partir del conjunto de entrenamiento y no del conjunto de validación/test. Esto evita la “fuga de información” en nuestro modelo, dándonos puntajes de validación/prueba inflados artificialmente.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:23.312139Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:23.299580Z&quot;}" data-execution_count="154">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unique tokens in source (de) vocabulary: </span><span class="sc">{</span><span class="bu">len</span>(SRC.vocab)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unique tokens in target (en) vocabulary: </span><span class="sc">{</span><span class="bu">len</span>(TRG.vocab)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Unique tokens in source (de) vocabulary: 7853
Unique tokens in target (en) vocabulary: 5893</code></pre>
</div>
</div>
<p>El paso final de preparar los datos es crear los iteradores. Estos se pueden iterar para devolver un lote de datos que tendrá un atributo <code>src</code> (los tensores de PyTorch que contienen un lote de oraciones de origen numeradas) y un atributo <code>trg</code> (los tensores de PyTorch que contienen un batch de oraciones de destino numeradas). “Numericalized” es solo una forma elegante de decir que se han convertido de una secuencia de tokens legibles a una secuencia de índices correspondientes, usando el vocabulario.</p>
<p>También necesitamos definir un dispositivo <code>torch.device</code>. Esto se usa para indicarle a torchText que coloque o no los tensores en la GPU. Usamos la función <code>torch.cuda.is_available()</code>, que devolverá True si se detecta una GPU en nuestra computadora. Pasamos este dispositivo al iterador.</p>
<p>Cuando obtenemos un lote de ejemplos usando un iterador, debemos asegurarnos de que todas las oraciones de origen tengan la misma longitud, al igual que las oraciones de destino. ¡Afortunadamente, los iteradores de torchText manejan esto por nosotros!</p>
<p>Usamos un <code>BucketIterator</code> en lugar del <code>Iterador</code> estándar, ya que crea lotes de tal manera que minimiza la cantidad de padding en las oraciones de origen y de destino.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:23.716604Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:23.695697Z&quot;}" data-execution_count="155">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>cpu</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:29.661585Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:29.630173Z&quot;}" data-execution_count="156">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>train_iterator, valid_iterator, test_iterator <span class="op">=</span> BucketIterator.splits(</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    (train_data, valid_data, test_data), </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> BATCH_SIZE, </span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="construyendo-el-modelo-seq2seq" class="level3">
<h3 class="anchored" data-anchor-id="construyendo-el-modelo-seq2seq">Construyendo el Modelo Seq2Seq</h3>
<p>Vamos a definir nuestro modelo en tres partes, el encoder, el decoder y el modelo Seq2Seq. Este ultimo encapsulará el proceso y transferencia entre los primeros dos.</p>
<section id="encoder" class="level4">
<h4 class="anchored" data-anchor-id="encoder">Encoder</h4>
<p>Primero, el encoder, es un LSTM de 2 capas. El paper que estamos implementando usa un LSTM de 4 capas, pero en favor del tiempo de entrenamiento lo reducimos a 2 capas. El concepto de RNN multicapa es fácil de expandir de 2 a 4 capas.</p>
<p>Para un RNN multicapa, la oración de entrada, <span class="math inline">\(X\)</span>, después de ser embeddida va a la primera capa (inferior) del RNN y los estados ocultos, <span class="math inline">\(H=\{h_1, h_2, ..., h_T\}\)</span> , la salida de esta capa se utiliza como entrada a la RNN en la capa superior. Así, representando cada capa con un superíndice, los hidden states en la primera capa vienen dados por:</p>
<p><span class="math display">\[h_t^1 = \text{EncoderRNN}^1(e(x_t), h_{t-1}^1)\]</span></p>
<p>Las hidden states en la segunda layer son dadas por:</p>
<p><span class="math display">\[h_t^2 = \text{EncoderRNN}^2(h_t^1, h_{t-1}^2)\]</span></p>
<p>El uso de un RNN multicapa también significa que también necesitaremos un hidden state inicial como entrada por capa, <span class="math inline">\(h_0^l\)</span>, y también generaremos un vector de contexto por capa, <span class="math inline">\(z^l\)</span>.</p>
<p>Si desean repasar un poco sobre LSTM pueden consultar este [enlance] (https://colah.github.io/posts/2015-08-Understanding-LSTMs/) Para este laboratorio, es suficiente que recuerden que lo que necesitamos saber es los LSTM, en lugar de simplemente tomar un estado oculto y devolver un nuevo estado oculto por paso de tiempo, también toman y devuelven un <em>estado de celda</em>, <span class="math inline">\(c_t\)</span>, por paso de tiempo.</p>
<p><span class="math display">\[\begin{align*}
h_t &amp;= \text{RNN}(e(x_t), h_{t-1})\\
(h_t, c_t) &amp;= \text{LSTM}(e(x_t), h_{t-1}, c_{t-1})
\end{align*}\]</span></p>
<p>Podemos pensar en <span class="math inline">\(c_t\)</span> como otro tipo de hidden state. Similar a <span class="math inline">\(h_0^l\)</span>, <span class="math inline">\(c_0^l\)</span> se inicializará en un tensor de ceros. Además, nuestro vector de contexto ahora será tanto el hidden state final como el estado de celda final, es decir, <span class="math inline">\(z^l = (h_T^l, c_T^l)\)</span>.</p>
<p>Al extender nuestras ecuaciones multicapa a LSTM, obtenemos:</p>
<p><span class="math display">\[\begin{align*}
(h_t^1, c_t^1) &amp;= \text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))\\
(h_t^2, c_t^2) &amp;= \text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))
\end{align*}\]</span></p>
<p>Observen cómo solo nuestro hidden state de la primera capa se pasa como entrada a la segunda capa, y no el estado de la celda.</p>
<p>Así que nuestro codificador se parece a esto:</p>
<p>IMAGEN</p>
<p>Creamos esto en el código creando un módulo <code>Encoder</code>, que requiere que heredemos de <code>torch.nn.Module</code> y usemos <code>super().__init__()</code> como un código repetitivo. El codificador toma los siguientes argumentos: - <code>input_dim</code> es el tamaño/dimensionalidad de los vectores one-hot que se ingresarán al codificador. Esto es igual al tamaño del vocabulario de entrada (fuente). - <code>emb_dim</code> es la dimensionalidad de la capa de embedding. Esta capa convierte los vectores one-hot en vectores densos con dimensiones <code>emb_dim</code>. - <code>hid_dim</code> es la dimensionalidad de los estados ocultos y de celda. - <code>n_layers</code> es el número de capas en el RNN. - <code>dropout</code> es la cantidad de abandono a utilizar. Este es un parámetro de regularización para evitar el overfitting. Consulte [aqui] (https://www.coursera.org/lecture/deep-neural-network/understanding-dropout-YaGbR) para obtener más detalles sobre dropout.</p>
<p>No vamos a discutir la capa de embedding en detalle durante aqui pues ya lo hicimos previamente. Todo lo que necesitamos saber es que hay un paso antes de que las palabras (técnicamente, los índices de las palabras) pasen al RNN, donde las palabras se transforman en vectores. Para leer más sobre embedding de palabras, consulten estos artículos: <a href="https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/">1</a>, <a href="http://p.migdal.pl%20/2017/01/06/rey-hombre-mujer-reina-por%20qué.html">2</a>, <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">3</a>, <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">4</a>.</p>
<p>La capa de embedding se crea usando <code>nn.Embedding</code>, el LSTM con <code>nn.LSTM</code> y una capa de dropout con <code>nn.Dropout</code>. Consulten la [documentación de PyTorch ] (https://pytorch.org/docs/stable/nn.html) para obtener más información al respecto.</p>
<p>Una cosa a tener en cuenta es que el argumento <code>dropout</code> para el LSTM es cuánto dropout aplicar entre las capas de un RNN multicapa, es decir, entre la salida de estados ocultos de la capa <span class="math inline">\(l\)</span> y esos mismos estados ocultos que se utilizan para el entrada de la capa <span class="math inline">\(l+1\)</span>.</p>
<p>En el método <code>forward</code>, pasamos la oración fuente, <span class="math inline">\(X\)</span>, que se convierte en vectores densos usando la capa <code>embedding</code>, y luego se aplica el dropout. Estos embedding luego se pasan a la RNN. A medida que pasamos una secuencia completa a la RNN, ¡automáticamente hará el cálculo recurrente de los estados ocultos en toda la secuencia por nosotros! Tenga en cuenta que no pasamos un estado inicial oculto o de celda al RNN. Esto se debe a que, como se indica en la <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM">documentación</a>, si no se pasa ningún estado de celda/oculto a la RNN, crea automáticamente un estado inicial de celda/oculto como un tensor de ceros.</p>
<p>El RNN devuelve: <code>outputs</code> (el hidden state de la capa superior para cada paso de tiempo), <code>hidden</code> (el hidden state final para cada capa, <span class="math inline">\(h_T\)</span>, apiladas una encima de la otra) y <code>cell</code> (la estado de celda final para cada capa, <span class="math inline">\(c_T\)</span>, apilados uno encima del otro).</p>
<p>Como solo necesitamos los hidden state y de celda finales (para hacer nuestro vector de contexto), <code>forward</code> solo devuelve <code>hidden</code> y <code>cell</code>.</p>
<p>Los tamaños de cada uno de los tensores se dejan como comentarios en el código. En esta implementación, <code>n_directions</code> siempre será 1, sin embargo, tengan en cuenta que los RNN bidireccionales (cubiertos en el tutorial 3) tendrán <code>n_directions</code> como 2.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:30.685603Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:30.659165Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;24a7fe486fef7641af76e0308c8553e5&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-819cfe4960d74aaf&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="157">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, emb_dim, hid_dim, n_layers, dropout):</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hid_dim <span class="op">=</span> hid_dim</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_layers <span class="op">=</span> n_layers</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para </span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.embedding = </span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(input_dim, emb_dim)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para </span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.rnn = </span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(emb_dim, hid_dim, n_layers, dropout<span class="op">=</span>dropout)</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src):</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src = [src len, batch size]</span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(src))</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">#embedded = [src len, batch size, emb dim]</span></span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>        outputs, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.rnn(embedded)</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">#outputs = [src len, batch size, hid dim * n directions]</span></span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">#hidden = [n layers * n directions, batch size, hid dim]</span></span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">#cell = [n layers * n directions, batch size, hid dim]</span></span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a>        <span class="co">#outputs are always from the top hidden layer</span></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> hidden, cell</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="decoder" class="level4">
<h4 class="anchored" data-anchor-id="decoder">Decoder</h4>
<p>Ahora pasaremos a construir el decoder, el cual también será una 2-layer (4 en el paper) LSTM.</p>
<p><img src="assets/seq2seq3.png" class="img-fluid"></p>
<p>La clase <code>Decoder</code> hace un solo paso de decodificación, es decir, genera un solo token por paso. La primera capa recibirá un hidden state y de celda del paso de tiempo anterior, <span class="math inline">\((s_{t-1}^1, c_{t-1}^1)\)</span>, y lo alimenta a través del LSTM con el token incrustado actual, <span class="math inline">\(y_t\)</span>, para producir un nuevo hidden state y de celda, <span class="math inline">\((s_t ^1, c_t^1)\)</span>. Las capas subsiguientes usarán el estado oculto de la capa inferior, <span class="math inline">\(s_t^{l-1}\)</span>, y los estados ocultos y de celda anteriores de su capa, <span class="math inline">\((s_{t-1}^l, c_{t-1) }^l)\)</span>. Esto proporciona ecuaciones muy similares a las del codificador.</p>
<p><span class="math display">\[\begin{align*}
(s_t^1, c_t^1) = \text{DecoderLSTM}^1(d(y_t), (s_{t-1}^1, c_{t-1}^1))\\
(s_t^2, c_t^2) = \text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))
\end{align*}\]</span></p>
<p>Recuerde que los estados iniciales ocultos y de celda de nuestro decoder son nuestros vectores de contexto, que son los estados finales ocultos y de celda de nuestro decoder de la misma capa, es decir, <span class="math inline">\((s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)\)</span>.</p>
<p>Luego pasamos el hidden state desde la capa superior del RNN, <span class="math inline">\(s_t^L\)</span>, a través de una capa lineal, <span class="math inline">\(f\)</span>, para hacer una predicción de cuál será el siguiente token en la secuencia de destino (salida). debería ser, <span class="math inline">\(\hat{y}_{t+1}\)</span>.</p>
<p><span class="math display">\[\sombrero{y}_{t+1} = f(s_t^L)\]</span></p>
<p>Los argumentos y la inicialización son similares a la clase <code>Encoder</code>, excepto que ahora tenemos un <code>output_dim</code> que es el tamaño del vocabulario para la salida/objetivo. También está la adición de la capa ‘Lineal’, utilizada para hacer las predicciones desde el hidden state de la capa superior.</p>
<p>Dentro del método <code>forward</code>, aceptamos un batch de tokens de entrada, hidden state anteriores y estados de celda anteriores. Como solo estamos decodificando un token a la vez, los tokens de entrada siempre tendrán una longitud de secuencia de 1. “Aflojamos” los tokens de entrada para agregar una dimensión de longitud de oración de 1. Luego, de forma similar al encoder, pasamos a través de una capa de embedding y aplicamos dropout. Este batch de tokens embeddidos luego se pasa al RNN con los estados ocultos y de celda anteriores. Esto produce una “salida” (hidden state de la capa superior de la RNN), un nuevo “hidden state” (uno para cada capa, apilados uno encima del otro) y una nueva “celda”. estado (también uno por capa, apilados uno encima del otro). Luego pasamos la <code>salida</code> (después de deshacernos de la dimensión de longitud de la oración) a través de la capa lineal para recibir nuestra <code>predicción</code>. Luego devolvemos la <code>predicción</code>, el nuevo hidden state y el nuevo estado <code>celular</code>.</p>
<p><strong>Nota</strong>: como siempre tenemos una longitud de secuencia de 1, podríamos usar <code>nn.LSTMCell</code>, en lugar de <code>nn.LSTM</code>, ya que está diseñado para manejar un lote de entradas que no son necesariamente en una secuencia. <code>nn.LSTMCell</code> es solo una sola celda y <code>nn.LSTM</code> es un envoltorio alrededor de múltiples celdas potenciales. Usando <code>nn.LSTMCell</code> en este caso significaría que no tenemos que <code>descomprimir</code> para agregar una dimensión de longitud de secuencia falsa, pero necesitaríamos un <code>nn.LSTMCell</code> por capa en el decoder y para asegurar que cada <code>nn.LSTMCell</code> recibe el hidden state inicial correcto del codificador. Todo esto hace que el código sea menos conciso, de ahí la decisión de seguir con el <code>nn.LSTM</code> regular.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:33.673863Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:33.642635Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;9062a639f1c3bd604869ed020a65ea7e&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-84131f43444e74fa&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="158">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, output_dim, emb_dim, hid_dim, n_layers, dropout):</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 3 lineas para</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.output_dim = </span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.hid_dim =</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.n_layers = </span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_dim <span class="op">=</span> output_dim</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hid_dim <span class="op">=</span> hid_dim</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_layers <span class="op">=</span> n_layers</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para </span></span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.embedding = </span></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(output_dim, emb_dim)</span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para </span></span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.rnn = </span></span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(emb_dim, hid_dim, n_layers, dropout<span class="op">=</span>dropout)</span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_out <span class="op">=</span> nn.Linear(hid_dim, output_dim)</span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>, hidden, cell):</span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">#input = [batch size]</span></span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">#hidden = [n layers * n directions, batch size, hid dim]</span></span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#cell = [n layers * n directions, batch size, hid dim]</span></span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">#n directions in the decoder will both always be 1, therefore:</span></span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">#hidden = [n layers, batch size, hid dim]</span></span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#context = [n layers, batch size, hid dim]</span></span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> <span class="bu">input</span>.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb55-40"><a href="#cb55-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-41"><a href="#cb55-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#input = [1, batch size]</span></span>
<span id="cb55-42"><a href="#cb55-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-43"><a href="#cb55-43" aria-hidden="true" tabindex="-1"></a>        embedded <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.embedding(<span class="bu">input</span>))</span>
<span id="cb55-44"><a href="#cb55-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-45"><a href="#cb55-45" aria-hidden="true" tabindex="-1"></a>        <span class="co">#embedded = [1, batch size, emb dim]</span></span>
<span id="cb55-46"><a href="#cb55-46" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb55-47"><a href="#cb55-47" aria-hidden="true" tabindex="-1"></a>        output, (hidden, cell) <span class="op">=</span> <span class="va">self</span>.rnn(embedded, (hidden, cell))</span>
<span id="cb55-48"><a href="#cb55-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-49"><a href="#cb55-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = [seq len, batch size, hid dim * n directions]</span></span>
<span id="cb55-50"><a href="#cb55-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">#hidden = [n layers * n directions, batch size, hid dim]</span></span>
<span id="cb55-51"><a href="#cb55-51" aria-hidden="true" tabindex="-1"></a>        <span class="co">#cell = [n layers * n directions, batch size, hid dim]</span></span>
<span id="cb55-52"><a href="#cb55-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-53"><a href="#cb55-53" aria-hidden="true" tabindex="-1"></a>        <span class="co">#seq len and n directions will always be 1 in the decoder, therefore:</span></span>
<span id="cb55-54"><a href="#cb55-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = [1, batch size, hid dim]</span></span>
<span id="cb55-55"><a href="#cb55-55" aria-hidden="true" tabindex="-1"></a>        <span class="co">#hidden = [n layers, batch size, hid dim]</span></span>
<span id="cb55-56"><a href="#cb55-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">#cell = [n layers, batch size, hid dim]</span></span>
<span id="cb55-57"><a href="#cb55-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-58"><a href="#cb55-58" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> <span class="va">self</span>.fc_out(output.squeeze(<span class="dv">0</span>))</span>
<span id="cb55-59"><a href="#cb55-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-60"><a href="#cb55-60" aria-hidden="true" tabindex="-1"></a>        <span class="co">#prediction = [batch size, output dim]</span></span>
<span id="cb55-61"><a href="#cb55-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-62"><a href="#cb55-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> prediction, hidden, cell</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="seq2seq" class="level3">
<h3 class="anchored" data-anchor-id="seq2seq">Seq2Seq</h3>
<p>Para la parte final de la implementación, implementaremos el modelo seq2seq. Esto manejará: - recibir la oración de entrada/fuente - usar el encoder para producir los vectores de contexto - usar el decoder para producir la salida predicha/oración objetivo</p>
<p>Nuestro modelo completo se verá así:</p>
<p><img src="activos/seq2seq4.png" class="img-fluid"></p>
<p>El modelo <code>Seq2Seq</code> incluye un <code>Encoder</code>, un <code>Decoder</code> y un <code>dispositivo</code> (usado para colocar tensores en la GPU, si existe).</p>
<p>Para esta implementación, debemos asegurarnos de que el número de capas y las dimensiones ocultas (y de celda) sean iguales en el ‘Encoder’ y ‘Decoder’. Este no es siempre el caso, no necesariamente necesitamos la misma cantidad de capas o los mismos tamaños de dimensiones ocultas en un modelo de sequence to sequence. Sin embargo, si hiciéramos algo como tener un número diferente de capas, tendríamos que tomar decisiones sobre cómo manejar esto. Por ejemplo, si nuestro encoder tiene 2 capas y nuestro decoder solo tiene 1, ¿cómo se maneja esto? ¿Promediamos los dos vectores de contexto generados por el decoder? ¿Pasamos ambos por una capa lineal? ¿Solo usamos el vector de contexto de la capa más alta? Etc.</p>
<p>Nuestro método “forward” toma la oración fuente, la oración objetivo y un ratio de teacher-forcing. El ratio de teacher-forcing se usa cuando entrenamos nuestro modelo. Al decodificar, en cada paso, predeciremos cuál será el próximo token en la secuencia de destino de los tokens anteriores decodificados, <span class="math inline">\(\hat{y}_{t+1}=f(s_t^L)\)</span>. Con una probabilidad igual a la tasa de teacher forcing (<code>teacher_forcing_ratio</code>), utilizaremos el siguiente token real de la secuencia como entrada al decoder durante el siguiente paso. Sin embargo, con probabilidad <code>1 - Teacher_forcing_ratio</code>, usaremos el token que el modelo predijo como la próxima entrada al modelo, incluso si no coincide con el siguiente token real en la secuencia.</p>
<p>Lo primero que hacemos en el método <code>forward</code> es crear un tensor <code>outputs</code> que almacenará todas nuestras predicciones, <span class="math inline">\(\hat{Y}\)</span>.</p>
<p>Luego alimentamos la oración de entrada/fuente, <code>src</code>, en el encoder y recibimos los estados ocultos y de celda finales.</p>
<p>La primera entrada al decoder es el token de inicio de secuencia (<code>&lt;sos&gt;</code>). Como nuestro tensor <code>trg</code> ya tiene el token <code>&lt;sos&gt;</code> agregado (desde cuando definimos el <code>init_token</code> en nuestro campo <code>TRG</code>) obtenemos nuestro <span class="math inline">\(y_1\)</span> cortándolo. Sabemos qué tan largas deben ser nuestras oraciones de destino (<code>max_len</code>), por lo que las repetimos muchas veces. El último token ingresado en el decoder es el <strong>antes</strong> del token <code>&lt;eos&gt;</code> - el <code>&lt;eos&gt;</code> el token nunca se ingresa en el decoder.</p>
<p>Durante cada iteración del ciclo, nosotros: - pasar la entrada, los estados de celda anteriores ocultos y anteriores (<span class="math inline">\(y_t, s_{t-1}, c_{t-1}\)</span>) al decoder - recibir una predicción, el siguiente estado oculto y el siguiente estado de celda (<span class="math inline">\(\hat{y}_{t+1}, s_{t}, c_{t}\)</span>) del decoder - colocar nuestra predicción, <span class="math inline">\(\hat{y}_{t+1}\)</span>/<code>output</code> en nuestro tensor de predicciones, <span class="math inline">\(\hat{Y}\)</span>/<code>outputs</code> - decidir si vamos a “fuerza de maestros” o no - si lo hacemos, la siguiente ‘entrada’ es el siguiente token de verdad fundamental en la secuencia, <span class="math inline">\(y_{t+1}\)</span>/<code>trg[t]</code> - si no lo hacemos, la siguiente <code>entrada</code> es el siguiente token predicho en la secuencia, <span class="math inline">\(\hat{y}_{t+1}\)</span>/<code>top1</code>, que obtenemos al hacer un <code>argmax</code> sobre el tensor de salida</p>
<p>Una vez que hemos hecho todas nuestras predicciones, devolvemos nuestro tensor lleno de predicciones, <span class="math inline">\(\hat{Y}\)</span>/<code>outputs</code>.</p>
<p><strong>Nota</strong>: nuestro ccilo decodificador comienza en 1, no en 0. Esto significa que el elemento 0 de nuestro tensor de <code>salidas</code> sigue siendo todo ceros. Así que nuestras <code>trg</code> y <code>outputs</code> se parecen a:</p>
<p><span class="math display">\[\begin{alinear*}
\text{trg} = [&lt;sos&gt;, &amp;y_1, y_2, y_3, &lt;eos&gt;]\\
\text{resultados} = [0, &amp;\hat{y}_1, \hat{y}_2, \hat{y}_3, &lt;eos&gt;]
\end{align*}\]</span></p>
<p>Posteriormente cuando calculamos la pérdida, cortamos el primer elemento de cada tensor para obtener:</p>
<p><span class="math display">\[\begin{alinear*}
\text{trg} = [&amp;y_1, y_2, y_3, &lt;eos&gt;]\\
\text{salidas} = [&amp;\hat{y}_1, \hat{y}_2, \hat{y}_3, &lt;eos&gt;]
\end{align*}\]</span></p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:34.696215Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:34.680165Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;a053dcb6ba362103fad11691a8c9cdfd&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-3cf708a546f162a5&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="159">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Seq2Seq(nn.Module):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, decoder, device):</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> encoder.hid_dim <span class="op">==</span> decoder.hid_dim, <span class="op">\</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Hidden dimensions of encoder and decoder must be equal!"</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> encoder.n_layers <span class="op">==</span> decoder.n_layers, <span class="op">\</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Encoder and decoder must have equal number of layers!"</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, src, trg, teacher_forcing_ratio <span class="op">=</span> <span class="fl">0.5</span>):</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">#src = [src len, batch size]</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [trg len, batch size]</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">#teacher_forcing_ratio is probability to use teacher forcing</span></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">#e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time</span></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> trg.shape[<span class="dv">1</span>]</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>        trg_len <span class="op">=</span> trg.shape[<span class="dv">0</span>]</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>        trg_vocab_size <span class="op">=</span> <span class="va">self</span>.decoder.output_dim</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">#tensor to store decoder outputs</span></span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> torch.zeros(trg_len, batch_size, trg_vocab_size).to(<span class="va">self</span>.device)</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">#last hidden state of the encoder is used as the initial hidden state of the decoder</span></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>        hidden, cell <span class="op">=</span> <span class="va">self</span>.encoder(src)</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">#first input to the decoder is the &lt;sos&gt; tokens</span></span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">input</span> <span class="op">=</span> trg[<span class="dv">0</span>,:]</span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, trg_len):</span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>            <span class="co">#insert input token embedding, previous hidden and previous cell states</span></span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a>            <span class="co">#receive output tensor (predictions) and new hidden and cell states</span></span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Aprox 1 linea para </span></span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># output, hidden, cell =</span></span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># YOUR CODE HERE</span></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a>            output, hidden, cell <span class="op">=</span> <span class="va">self</span>.decoder(<span class="bu">input</span>, hidden, cell)</span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>            <span class="co">#place predictions in a tensor holding predictions for each token</span></span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a>            outputs[t] <span class="op">=</span> output</span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a>            <span class="co">#decide if we are going to use teacher forcing or not</span></span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a>            teacher_force <span class="op">=</span> random.random() <span class="op">&lt;</span> teacher_forcing_ratio</span>
<span id="cb56-49"><a href="#cb56-49" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-50"><a href="#cb56-50" aria-hidden="true" tabindex="-1"></a>            <span class="co">#get the highest predicted token from our predictions</span></span>
<span id="cb56-51"><a href="#cb56-51" aria-hidden="true" tabindex="-1"></a>            top1 <span class="op">=</span> output.argmax(<span class="dv">1</span>) </span>
<span id="cb56-52"><a href="#cb56-52" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-53"><a href="#cb56-53" aria-hidden="true" tabindex="-1"></a>            <span class="co">#if teacher forcing, use actual next token as next input</span></span>
<span id="cb56-54"><a href="#cb56-54" aria-hidden="true" tabindex="-1"></a>            <span class="co">#if not, use predicted token</span></span>
<span id="cb56-55"><a href="#cb56-55" aria-hidden="true" tabindex="-1"></a>            <span class="bu">input</span> <span class="op">=</span> trg[t] <span class="cf">if</span> teacher_force <span class="cf">else</span> top1</span>
<span id="cb56-56"><a href="#cb56-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-57"><a href="#cb56-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-seq2seq-model" class="level3">
<h3 class="anchored" data-anchor-id="training-seq2seq-model">Training Seq2Seq Model</h3>
<p>Ahora que tenemos nuestro modelo implementado, podemos comenzar a entrenarlo.</p>
<p>Primero, inicializaremos nuestro modelo. Como se mencionó anteriormente, las dimensiones de entrada y salida están definidas por el tamaño del vocabulario. Las dimensiones de embedding y el dropout del encoder y el decoder pueden ser diferentes, pero el número de capas y el tamaño de los estados ocultos/de celda deben ser los mismos.</p>
<p>Luego definimos el encoder, el decoder y luego nuestro modelo Seq2Seq, que colocamos en el “device”.</p>
<p>El siguiente paso es inicializar los pesos de nuestro modelo. En el paper afirman que inicializan todos los pesos a partir de una distribución uniforme entre -0,08 y +0,08, es decir, <span class="math inline">\(\mathcal{U}(-0,08, 0,08)\)</span>.</p>
<p>Inicializamos los pesos en PyTorch creando una función que “aplicamos” a nuestro modelo. Al usar <code>apply</code>, se llamará a la función <code>init_weights</code> en cada módulo y submódulo dentro de nuestro modelo. Para cada módulo, recorremos todos los parámetros y los muestreamos desde una distribución uniforme con <code>nn.init.uniform_</code>.</p>
<p>También definimos una función que calculará el número de parámetros entrenables en el modelo.</p>
<p>Definimos nuestro optimizador, que usamos para actualizar nuestros parámetros en el ciclo de entrenamiento. Consulte <a href="http://ruder.io/optimizing-gradient-descent/">esta publicación</a> para obtener información sobre diferentes optimizadores. Aquí usaremos a Adam</p>
<p>A continuación, definimos nuestra función de pérdida. La función <code>CrossEntropyLoss</code> calcula tanto el log softmax como la log-likelihood negativo de nuestras predicciones.</p>
<p>Nuestra función de pérdida calcula la pérdida promedio por token, sin embargo, al pasar el índice del token <code>&lt;pad&gt;</code> como el argumento <code>ignore_index</code>, ignoramos la pérdida siempre que el token de destino sea un token de relleno (padding).</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:41.167607Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:35.024487Z&quot;}" data-execution_count="160">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>INPUT_DIM <span class="op">=</span> <span class="bu">len</span>(SRC.vocab)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>OUTPUT_DIM <span class="op">=</span> <span class="bu">len</span>(TRG.vocab)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>ENC_EMB_DIM <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>DEC_EMB_DIM <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>HID_DIM <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>N_LAYERS <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>ENC_DROPOUT <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>DEC_DROPOUT <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>enc <span class="op">=</span> Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>dec <span class="op">=</span> Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Seq2Seq(enc, dec, device).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:41.857202Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:41.472920Z&quot;}" data-execution_count="161">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weights(m):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> m.named_parameters():</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>        nn.init.uniform_(param.data, <span class="op">-</span><span class="fl">0.08</span>, <span class="fl">0.08</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">apply</span>(init_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="161">
<pre><code>Seq2Seq(
  (encoder): Encoder(
    (embedding): Embedding(7853, 256)
    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (decoder): Decoder(
    (embedding): Embedding(5893, 256)
    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)
    (fc_out): Linear(in_features=512, out_features=5893, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:42.370872Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:42.339409Z&quot;}" data-execution_count="162">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_parameters(model):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'El modelo tiene </span><span class="sc">{</span>count_parameters(model)<span class="sc">:,}</span><span class="ss"> parametros entrenables'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>El modelo tiene 13,898,501 parametros entrenables</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:42.863269Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:42.847270Z&quot;}" data-execution_count="163">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:43.263371Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:43.247371Z&quot;}" data-execution_count="164">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>TRG_PAD_IDX <span class="op">=</span> TRG.vocab.stoi[TRG.pad_token]</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss(ignore_index <span class="op">=</span> TRG_PAD_IDX)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A continuación, definiremos nuestro ciclo de entrenamiento.</p>
<p>Primero, configuraremos el modelo en “modo de entrenamiento” con <code>model.train()</code>. Esto activará el dropout (y batch normalization, que no estamos usando) y luego iterará a través de nuestro iterador de datos.</p>
<p>Como se indicó anteriormente, nuestro ciclo decodificador comienza en 1, no en 0. Esto significa que el elemento 0 de nuestro tensor de “salidas” sigue siendo todo ceros. Así que nuestras <code>trg</code> y <code>outputs</code> se parecen a:</p>
<p><span class="math display">\[\begin{alinear*}
\text{trg} = [&lt;sos&gt;, &amp;y_1, y_2, y_3, &lt;eos&gt;]\\
\text{resultados} = [0, &amp;\hat{y}_1, \hat{y}_2, \hat{y}_3, &lt;eos&gt;]
\end{align*}\]</span></p>
<p>Aquí, cuando calculamos la pérdida, cortamos el primer elemento de cada tensor para obtener:</p>
<p><span class="math display">\[\begin{alinear*}
\text{trg} = [&amp;y_1, y_2, y_3, &lt;eos&gt;]\\
\text{salidas} = [&amp;\hat{y}_1, \hat{y}_2, \hat{y}_3, &lt;eos&gt;]
\end{align*}\]</span></p>
<p>En cada iteración: - obtener las oraciones de origen y de destino del lote, <span class="math inline">\(X\)</span> y <span class="math inline">\(Y\)</span> - poner a cero los gradientes calculados a partir del último lote - introduzca el origen y el destino en el modelo para obtener el resultado, <span class="math inline">\(\hat{Y}\)</span> - como la función de pérdida solo funciona en entradas 2d con objetivos 1d, necesitamos aplanar cada uno de ellos con <code>.view</code> - cortamos la primera columna de los tensores de salida y destino como se mencionó anteriormente - calcula los gradientes con <code>loss.backward()</code> - recorte los gradientes para evitar que exploten (un problema común en RNN) - actualizar los parámetros de nuestro modelo haciendo un paso optimizador - sumar el valor de la pérdida a un total acumulado</p>
<p>Finalmente, devolvemos la pérdida que se promedia en todos los batches.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:43.863678Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:43.847678Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;6ae8296e47d370619304a765919b81b2&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-5e78bda9de1a9bb9&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="165">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, iterator, optimizer, criterion, clip):</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(iterator):</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>        src <span class="op">=</span> batch.src</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> batch.trg</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># optimizer.zero...</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(src, trg)</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [trg len, batch size]</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = [trg len, batch size, output dim]</span></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>        output_dim <span class="op">=</span> output.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output[<span class="dv">1</span>:].view(<span class="op">-</span><span class="dv">1</span>, output_dim)</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>        trg <span class="op">=</span> trg[<span class="dv">1</span>:].view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">#trg = [(trg len - 1) * batch size]</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = [(trg len - 1) * batch size, output dim]</span></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para</span></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss = </span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, trg)</span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(iterator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nuestro ciclo de evaluación es similar a nuestro ciclo de entrenamiento, sin embargo, como no estamos actualizando ningún parámetro, no necesitamos pasar un optimizador o un valor de clip.</p>
<p>Debemos recordar poner el modelo en modo de evaluación con <code>model.eval()</code>. Esto desactivará el dropout (y la batch normalization, si se usa).</p>
<p>Usamos el bloque <code>with torch.no_grad()</code> para garantizar que no se calculen gradientes dentro del bloque. Esto reduce el consumo de memoria y acelera el proceso.</p>
<p>El ciclo de iteración es similar (sin las actualizaciones de parámetros); sin embargo, debemos asegurarnos de desactivar el forzado del maestro para la evaluación. } Esto hará que el modelo solo use sus propias predicciones para hacer más predicciones dentro de una oración, lo que refleja cómo se usaría en la implementación.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:44.241992Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:44.225998Z&quot;}" data-execution_count="166">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, iterator, criterion):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(iterator):</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>            src <span class="op">=</span> batch.src</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>            trg <span class="op">=</span> batch.trg</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(src, trg, <span class="dv">0</span>) <span class="co">#turn off teacher forcing</span></span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>            <span class="co">#trg = [trg len, batch size]</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>            <span class="co">#output = [trg len, batch size, output dim]</span></span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>            output_dim <span class="op">=</span> output.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> output[<span class="dv">1</span>:].view(<span class="op">-</span><span class="dv">1</span>, output_dim)</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>            trg <span class="op">=</span> trg[<span class="dv">1</span>:].view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>            <span class="co">#trg = [(trg len - 1) * batch size]</span></span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>            <span class="co">#output = [(trg len - 1) * batch size, output dim]</span></span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(output, trg)</span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>            epoch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(iterator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A continuación, crearemos una función que usaremos para decirnos cuánto tarda una época.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:10:44.738487Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:44.726276Z&quot;}" data-execution_count="167">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epoch_time(start_time, end_time):</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    elapsed_time <span class="op">=</span> end_time <span class="op">-</span> start_time</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    elapsed_mins <span class="op">=</span> <span class="bu">int</span>(elapsed_time <span class="op">/</span> <span class="dv">60</span>)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    elapsed_secs <span class="op">=</span> <span class="bu">int</span>(elapsed_time <span class="op">-</span> (elapsed_mins <span class="op">*</span> <span class="dv">60</span>))</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> elapsed_mins, elapsed_secs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ahora sí, ¡empecemos a entrenar a nuestro modelo!</p>
<p>En cada época, comprobaremos si nuestro modelo ha logrado la mejor pérdida de validación hasta el momento. Si es así, actualizaremos nuestra mejor pérdida de validación y guardaremos los parámetros de nuestro modelo (llamado <code>state_dict</code> en PyTorch). Luego, cuando lleguemos a probar nuestro modelo, usaremos los parámetros guardados para lograr la mejor pérdida de validación.</p>
<p>Estaremos mostrando tanto la pérdida como la perplejidad en cada época. Es más fácil ver un cambio en la perplejidad que un cambio en la pérdida ya que los números son mucho mayores.</p>
<p>Ademas, cargaremos los parámetros (<code>state_dict</code>) que dieron a nuestro modelo la mejor pérdida de validación y ejecutaremos el modelo en el conjunto de prueba.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:18:56.768139Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:10:46.646109Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;3abe75f3dec72dbef634b76eecb3cb54&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-c2a7405dde118a6e&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="168">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># para que pueda definir</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co"># N_EPOCHS = 3</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CLIP = 1</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>CLIP <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>best_valid_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(N_EPOCHS):</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> train(model, train_iterator, optimizer, criterion, CLIP)</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>    valid_loss <span class="op">=</span> evaluate(model, valid_iterator, criterion)</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>    epoch_mins, epoch_secs <span class="op">=</span> epoch_time(start_time, end_time)</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> valid_loss <span class="op">&lt;</span> best_valid_loss:</span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>        best_valid_loss <span class="op">=</span> valid_loss</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>        torch.save(model.state_dict(), <span class="st">'tut1-model.pt'</span>)</span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch: </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:02}</span><span class="ss"> | Time: </span><span class="sc">{</span>epoch_mins<span class="sc">}</span><span class="ss">m </span><span class="sc">{</span>epoch_secs<span class="sc">}</span><span class="ss">s'</span>)</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\t</span><span class="ss">Train Loss: </span><span class="sc">{</span>train_loss<span class="sc">:.3f}</span><span class="ss"> | Train PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(train_loss)<span class="sc">:7.3f}</span><span class="ss">'</span>)</span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\t</span><span class="ss"> Val. Loss: </span><span class="sc">{</span>valid_loss<span class="sc">:.3f}</span><span class="ss"> |  Val. PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(valid_loss)<span class="sc">:7.3f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 01 | Time: 12m 3s
    Train Loss: 5.063 | Train PPL: 158.101
     Val. Loss: 4.905 |  Val. PPL: 135.009
Epoch: 02 | Time: 12m 15s
    Train Loss: 4.486 | Train PPL:  88.742
     Val. Loss: 4.775 |  Val. PPL: 118.543
Epoch: 03 | Time: 12m 26s
    Train Loss: 4.211 | Train PPL:  67.421
     Val. Loss: 4.583 |  Val. PPL:  97.784
Epoch: 04 | Time: 12m 28s
    Train Loss: 3.999 | Train PPL:  54.548
     Val. Loss: 4.501 |  Val. PPL:  90.112
Epoch: 05 | Time: 12m 22s
    Train Loss: 3.849 | Train PPL:  46.940
     Val. Loss: 4.410 |  Val. PPL:  82.280
Epoch: 06 | Time: 12m 31s
    Train Loss: 3.736 | Train PPL:  41.915
     Val. Loss: 4.306 |  Val. PPL:  74.150</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T22:26:39.976008Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T22:26:39.653650Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;6f8a150031eecd843b79d77d31d64804&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-b70f37945f5a9981&quot;,&quot;locked&quot;:true,&quot;points&quot;:50,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="169">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Se valuara que el loss de training sea menor a 4 y el de validacion a 4.5</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">25</span>):        </span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(train_loss), <span class="st">"3c3d"</span>, <span class="st">'0x1.0000000000000p+2'</span>)</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">25</span>):        </span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(valid_loss), <span class="st">"3c3d"</span>, <span class="st">'0x1.2000000000000p+2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"25"}--> 
         ✓ [25 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"25"}--> 
         ✓ [25 marks] 
         </h1> </div>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-08-07T12:25:14.650820Z&quot;,&quot;start_time&quot;:&quot;2023-08-07T12:25:12.622015Z&quot;}" data-execution_count="170">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(<span class="st">'tut1-model.pt'</span>))</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>test_loss <span class="op">=</span> evaluate(model, test_iterator, criterion)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'| Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.3f}</span><span class="ss"> | Test PPL: </span><span class="sc">{</span>math<span class="sc">.</span>exp(test_loss)<span class="sc">:7.3f}</span><span class="ss"> |'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>| Test Loss: 4.317 | Test PPL:  74.960 |</code></pre>
</div>
</div>
<div class="cell" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;58fd9a560ef4d1a143e87ce331286237&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-e94ae9af3a4c26ff&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="171">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio"</span>)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>tick.summarise_marks() <span class="co"># </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio</code></pre>
</div>
<div class="cell-output cell-output-display">
<!--{id:"TOTALMARK",marks:"81", available:"81"}  -->
        
        <h1> 81 / 81 marks (100.0%) </h1>
        
</div>
</div>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>